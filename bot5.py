#ëª¨ë¸ ë°”ê¾¸ëŠ” ê¸°ëŠ¥ ë“¤ì–´ê°„ ë²„ì „

import os
import discord
from discord.ext import commands
from dotenv import load_dotenv
import google.generativeai as genai
import asyncio
import re # ì •ê·œ í‘œí˜„ì‹ ëª¨ë“ˆ ì¶”ê°€
import traceback # ì˜¤ë¥˜ ì¶”ì  ëª¨ë“ˆ ì¶”ê°€

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from prompt import SYSTEM_PROMPT
from weather import forecast_today, city_map # forecast_todayì™€ city_map ì„í¬íŠ¸

load_dotenv()

DISCORD_TOKEN = os.getenv('DISCORD_TOKEN')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

if not DISCORD_TOKEN or not GEMINI_API_KEY:
    print("DISCORD_TOKENê³¼ GEMINI_API_KEYë¥¼ .env íŒŒì¼ì— ë°˜ë“œì‹œ ì„¤ì •í•˜ì„¸ìš”!")
    exit(1)

# Google Gemini API ì„¤ì •
genai.configure(api_key=GEMINI_API_KEY)

intents = discord.Intents.default()
intents.message_content = True
intents.guilds = True

bot = commands.Bot(command_prefix='!', intents=intents)

# --- ëª¨ë¸ ì „í™˜ ë¡œì§ ê´€ë ¨ ì „ì—­ ë³€ìˆ˜ ---
MODEL_NAMES = [
    'gemini-2.5-flash-preview-05-20',
    'gemini-2.0-flash-001',
    'gemma-3-27b-it', 
    'gemini-1.5-flash-002'
]
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ í•„í„°ë§ (ì˜ˆ: Gemma ëª¨ë¸ì€ í˜„ì¬ genai ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì§ì ‘ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
# ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì€ genai.list_models() ë“±ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
# ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ì œê³µëœ ëª©ë¡ì„ ì‚¬ìš©í•˜ë˜, Gemma ëª¨ë¸ì€ ì£¼ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.

current_model_index = 0 # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ MODEL_NAMES ë¦¬ìŠ¤íŠ¸ì˜ ì¸ë±ìŠ¤
gemini_model = None # ì‹¤ì œ ëª¨ë¸ ê°ì²´ (ì „ì—­ìœ¼ë¡œ ê´€ë¦¬)

# --- ëª¨ë¸ ì„¤ì • ì „ì—­ ë³€ìˆ˜ ---
generation_config_global = genai.types.GenerationConfig(
    temperature=0.7,
    top_p=0.9,
    top_k=40,
    max_output_tokens=1000,
)
safety_settings_global = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
]
SYSTEM_PROMPT_GLOBAL = SYSTEM_PROMPT

# ì‚¬ìš©ìë³„ ëŒ€í™” ì„¸ì…˜ ì €ì¥ìš© (ë©”ëª¨ë¦¬ ê¸°ë°˜)
chat_sessions = {}

def initialize_model(model_idx_to_load: int):
    """
    ì§€ì •ëœ ì¸ë±ìŠ¤ì˜ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì „ì—­ ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    global gemini_model, current_model_index, chat_sessions

    if not (0 <= model_idx_to_load < len(MODEL_NAMES)):
        print(f"ì˜¤ë¥˜: ëª¨ë¸ ì¸ë±ìŠ¤ {model_idx_to_load}ê°€ MODEL_NAMES ë¦¬ìŠ¤íŠ¸ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")
        return False

    model_name_to_try = MODEL_NAMES[model_idx_to_load]
    print(f"ëª¨ë¸ ì´ˆê¸°í™”/ë³€ê²½ ì‹œë„: {model_name_to_try} (ì¸ë±ìŠ¤: {model_idx_to_load})")
    try:
        new_model = genai.GenerativeModel(
            model_name=model_name_to_try,
            system_instruction=SYSTEM_PROMPT_GLOBAL,
            generation_config=generation_config_global,
            safety_settings=safety_settings_global
        )
        gemini_model = new_model
        current_model_index = model_idx_to_load

        if chat_sessions:
            print(f"ëª¨ë¸ì´ '{model_name_to_try}'(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë“  ê¸°ì¡´ ì±„íŒ… ì„¸ì…˜ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            chat_sessions.clear()

        print(f"ì„±ê³µì ìœ¼ë¡œ ëª¨ë¸ì„ '{model_name_to_try}'(ìœ¼)ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        print(f"ëª¨ë¸ '{model_name_to_try}' ë¡œë“œ ì‹¤íŒ¨: {e}")
        if "The model `gemma" in str(e) and "is not found" in str(e):
            print(f"ì°¸ê³ : '{model_name_to_try}' ëª¨ë¸ì€ google.generativeai ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì§ì ‘ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Vertex AI ë˜ëŠ” ë‹¤ë¥¸ ë°©ë²•ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.")
        elif "API key not valid" in str(e):
            print("Gemini API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

def get_or_create_chat_session(user_id: str):
    if not gemini_model:
        print("ì˜¤ë¥˜: get_or_create_chat_session í˜¸ì¶œ ì‹œ gemini_modelì´ Noneì…ë‹ˆë‹¤. ëª¨ë¸ ì´ˆê¸°í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return None

    if user_id not in chat_sessions:
        try:
            chat_sessions[user_id] = gemini_model.start_chat(history=[])
            print(f"'{MODEL_NAMES[current_model_index]}' ëª¨ë¸ë¡œ ìƒˆë¡œìš´ ì±„íŒ… ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤: {user_id}")
        except Exception as e:
            print(f"ì±„íŒ… ì„¸ì…˜ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ({user_id}): {e}")
            return None
    return chat_sessions[user_id]

async def generate_response(user_id: str, user_message: str, retry_count=0, original_error_model_index_for_retry=None):
    """
    ë‚ ì”¨ ìš”ì²­ì„ ìš°ì„  ì²˜ë¦¬í•˜ê³ , ê·¸ ì™¸ì—ëŠ” Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    API í• ë‹¹ëŸ‰/ëª¨ë¸ ìƒíƒœ ë¬¸ì œ ë˜ëŠ” ê¸°íƒ€ ì˜¤ë¥˜ ë°œìƒ ì‹œ ëª¨ë¸ì„ ë³€ê²½í•˜ê³  ì¬ì‹œë„í•©ë‹ˆë‹¤.
    """
    global current_model_index, gemini_model

    # 1. ë‚ ì”¨ ìš”ì²­ ì²˜ë¦¬
    supported_cities_kr = list(city_map.keys())
    if not supported_cities_kr:
        print("ê²½ê³ : weather.pyì˜ city_mapì— ë„ì‹œê°€ ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë‚ ì”¨ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.")
    else:
        cities_pattern_group = "|".join(re.escape(city) for city in supported_cities_kr)
        weather_pattern = re.compile(rf"(?i)\b({cities_pattern_group})\b\s*(?:ì€|ëŠ”|ì´|ê°€|ì˜)?\s*ë‚ ì”¨")
        match = weather_pattern.search(user_message)
        if match:
            found_city_kr = match.group(1)
            print(f"ë‚ ì”¨ ìš”ì²­ ê°ì§€: '{found_city_kr}' (ì›ë³¸ ë©”ì‹œì§€: '{user_message}')")
            forecast_result = forecast_today(found_city_kr)
            return f"{forecast_result}\n{found_city_kr} ë‚ ì”¨ ì •ë³´ì˜€ì–´, ì„ ìƒ."
        elif user_message.strip().startswith("?") and "ë‚ ì”¨" in user_message and not user_message.startswith(bot.command_prefix):
            default_city = "ì„œìš¸"
            if default_city in city_map:
                print(f"ì¼ë°˜ ë‚ ì”¨ ìš”ì²­ ê°ì§€. ê¸°ë³¸ ë„ì‹œ '{default_city}'ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤. (ì›ë³¸: '{user_message}')")
                forecast_result = forecast_today(default_city)
                return f"ì–´ë–¤ ë„ì‹œì¸ì§€ ì •í™•íˆ ì•ˆ ì•Œë ¤ì¤˜ì„œ ì¼ë‹¨ {default_city} ë‚ ì”¨ë¥¼ ê°€ì ¸ì™”ì–´, ì„ ìƒ.\n{forecast_result}\në‹¤ë¥¸ ë„ì‹œê°€ ê¶ê¸ˆí•˜ë©´ 'ë„ì‹œì´ë¦„ ë‚ ì”¨'ë¼ê³  ë¬¼ì–´ë´."
            else:
                print(f"ê²½ê³ : ê¸°ë³¸ ë„ì‹œ '{default_city}'ê°€ city_mapì— ì—†ìŠµë‹ˆë‹¤.")
                return "ë‚ ì”¨ë¥¼ ì•Œë ¤ì£¼ê³  ì‹¶ì€ë°, ì–´ë–¤ ë„ì‹œì¸ì§€ ë§í•´ì¤„ë˜, ì„ ìƒ? ì˜ˆë¥¼ ë“¤ë©´ 'ì„œìš¸ ë‚ ì”¨' ì´ë ‡ê²Œ."

    # 2. ë‚ ì”¨ ìš”ì²­ì´ ì•„ë‹ˆë©´ Gemini APIì— ìœ„ì„
    if not gemini_model:
        print("ì˜¤ë¥˜: í˜„ì¬ í™œì„±í™”ëœ Gemini ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
        if retry_count == 0:
            print("ë´‡ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì¬ì´ˆê¸°í™”ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
            if initialize_model_globally():
                print("ëª¨ë¸ ì¬ì´ˆê¸°í™” ì„±ê³µ. ì‘ë‹µ ì¬ìƒì„± ì‹œë„.")
                return await generate_response(user_id, user_message, retry_count + 1)
            else:
                print("ëª¨ë¸ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨. ì‘ë‹µ ë¶ˆê°€.")
                return "ìœ¼ì•™... ì§€ê¸ˆì€ ë¨¸ë¦¬ê°€ ë„ˆë¬´ ì•„íŒŒì„œ ìƒê°í•  ìˆ˜ê°€ ì—†ì–´, ì„ ìƒ... ë‚˜ì¤‘ì— ë‹¤ì‹œ ë§ ê±¸ì–´ì¤˜."
        return "ìœ¼ì•™... ì§€ê¸ˆì€ ë¨¸ë¦¬ê°€ ë„ˆë¬´ ì•„íŒŒì„œ ìƒê°í•  ìˆ˜ê°€ ì—†ì–´, ì„ ìƒ... ë‚˜ì¤‘ì— ë‹¤ì‹œ ë§ ê±¸ì–´ì¤˜."

    chat_session = get_or_create_chat_session(user_id)
    if not chat_session:
        return "ìœ¼ìŒ... ì„ ìƒì´ë‘ ëŒ€í™” ì±„ë„ì„ ë§Œë“œëŠ” ë° ë­”ê°€ ë¬¸ì œê°€ ìƒê²¼ë‚˜ ë´. ì¡°ê¸ˆ ìˆë‹¤ ë‹¤ì‹œ ì‹œë„í•´ ì¤„ë˜?"

    try:
        print(f"'{MODEL_NAMES[current_model_index]}' ëª¨ë¸ì—ê²Œ ì „ë‹¬ (ID: {user_id}): {user_message}")
        gemini_response = await chat_session.send_message_async(user_message)

        if "API í• ë‹¹ëŸ‰" in gemini_response.text and "ëª¨ë¸ ìƒíƒœ" in gemini_response.text:
            print(f"ì‘ë‹µ ë‚´ìš©ì—ì„œ API í• ë‹¹ëŸ‰/ëª¨ë¸ ìƒíƒœ ë¬¸ì œ ê°ì§€: '{gemini_response.text}'")
            raise Exception("API í• ë‹¹ëŸ‰, ëª¨ë¸ ìƒíƒœ ë“±ì„ í™•ì¸í•´ì£¼ì„¸ìš”. (ì‘ë‹µ ë‚´ìš© ê¸°ë°˜)")

        return gemini_response.text

    except Exception as e:
        error_message = str(e)
        error_type = type(e).__name__
        print(f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ëª¨ë¸: {MODEL_NAMES[current_model_index]}, ì‚¬ìš©ì ID: {user_id}): {error_type} - {error_message}")
        traceback.print_exc()

        error_str_lower = error_message.lower()
        
        quota_error_keywords = ["quota", "rate limit", "resource exhausted", "api í• ë‹¹ëŸ‰", "ëª¨ë¸ ìƒíƒœ"]
        is_quota_or_model_issue = any(keyword in error_str_lower for keyword in quota_error_keywords)
        if "api í• ë‹¹ëŸ‰, ëª¨ë¸ ìƒíƒœ ë“±ì„ í™•ì¸í•´ì£¼ì„¸ìš”." in error_str_lower:
            is_quota_or_model_issue = True
        
        is_context_length_issue = "context length" in error_str_lower or "token" in error_str_lower or "size of the request" in error_str_lower
        is_safety_issue = "block" in error_str_lower or "safety" in error_str_lower
        is_api_key_issue = "api key not valid" in error_str_lower

        # ëª¨ë¸ ë³€ê²½ì„ ì‹œë„í•´ì•¼ í•˜ëŠ” ê²½ìš° (í• ë‹¹ëŸ‰ ë¬¸ì œ ë˜ëŠ” ê¸°íƒ€ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì˜¤ë¥˜)
        if is_quota_or_model_issue or not (is_context_length_issue or is_safety_issue or is_api_key_issue):
            if is_quota_or_model_issue:
                print(f"API í• ë‹¹ëŸ‰ ë˜ëŠ” ëª¨ë¸ ìƒíƒœ ë¬¸ì œ ê°ì§€. ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜ ì‹œë„ (í˜„ì¬ ëª¨ë¸: {MODEL_NAMES[current_model_index]}).")
            else:
                # ì´ê²ƒì´ 'ê¸°íƒ€ ì˜¤ë¥˜'ì— í•´ë‹¹í•©ë‹ˆë‹¤.
                print(f"ê¸°íƒ€ ì˜¤ë¥˜ ë°œìƒ (ì˜¤ë¥˜ ìœ í˜•: {error_type}, ë‚´ìš©: {error_message}). ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜ ì‹œë„ (í˜„ì¬ ëª¨ë¸: {MODEL_NAMES[current_model_index]}).")

            # ì¬ì‹œë„ ë£¨í”„ë¥¼ ìœ„í•œ ì‹œì‘ ëª¨ë¸ ì¸ë±ìŠ¤ ì„¤ì •
            # original_error_model_index_for_retryëŠ” ì´ ì¬ê·€ í˜¸ì¶œ ì²´ì¸ì—ì„œ ì²˜ìŒ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ëª¨ë¸ì˜ ì¸ë±ìŠ¤
            if original_error_model_index_for_retry is None:
                original_error_model_index_for_retry = current_model_index
            
            tried_models_in_this_cycle = 0
            
            # í˜„ì¬ ëª¨ë¸ ì¸ë±ìŠ¤ë¶€í„° ì‹œì‘í•˜ì—¬ ìˆœí™˜í•˜ë©° ë‹¤ìŒ ëª¨ë¸ ì‹œë„
            for i in range(len(MODEL_NAMES)):
                next_model_idx_candidate = (current_model_index + 1 + i) % len(MODEL_NAMES)
                
                # ëª¨ë“  ëª¨ë¸ì„ í•œ ë°”í€´ ë‹¤ ëŒì•˜ëŠ”ì§€ í™•ì¸ (í˜„ì¬ ëª¨ë¸ë¡œ ëŒì•„ì™”ê³ , ìµœì†Œ í•œ ë²ˆì€ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‹œë„í•œ ê²½ìš°)
                if next_model_idx_candidate == original_error_model_index_for_retry and tried_models_in_this_cycle > 0 :
                    print("ëª¨ë“  ê°€ìš© ëª¨ë¸ì„ ìˆœíšŒí•˜ë©° ì‹œë„í–ˆì§€ë§Œ ì˜¤ë¥˜ê°€ ì§€ì†ë©ë‹ˆë‹¤.")
                    break # ë£¨í”„ ì¢…ë£Œ, ì•„ë˜ì—ì„œ ìµœì¢… ì‹¤íŒ¨ ë©”ì‹œì§€ ë°˜í™˜

                print(f"ëª¨ë¸ ì „í™˜ ì‹œë„: {MODEL_NAMES[next_model_idx_candidate]}")
                if initialize_model(next_model_idx_candidate):
                    print(f"ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ {MODEL_NAMES[current_model_index]}(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ë©”ì‹œì§€ ì¬ì „ì†¡ ì‹œë„.")
                    # ëª¨ë¸ ë³€ê²½ í›„ì—ëŠ” ìƒˆ ì„¸ì…˜ì´ í•„ìš” (initialize_modelì—ì„œ chat_sessions.clear() í˜¸ì¶œ)
                    return await generate_response(user_id, user_message, retry_count + 1, original_error_model_index_for_retry)
                else:
                    print(f"{MODEL_NAMES[next_model_idx_candidate]} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ë‹¤ìŒ ëª¨ë¸ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                    tried_models_in_this_cycle += 1
                    # current_model_indexëŠ” initialize_model ì‹¤íŒ¨ ì‹œ ë³€ê²½ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
                    # ë‹¤ìŒ ë£¨í”„ì—ì„œ (current_model_index + 1 + i) % len(MODEL_NAMES)ëŠ” ë‹¤ìŒ ëª¨ë¸ì„ ê°€ë¦¬í‚´
            
            # ëª¨ë“  ëª¨ë¸ì„ ì‹œë„í–ˆê±°ë‚˜, ë” ì´ìƒ ì‹œë„í•  ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° (ë£¨í”„ë¥¼ ë¹ ì ¸ë‚˜ì˜¨ ê²½ìš°)
            print("ëª¨ë“  ê°€ìš© ëª¨ë¸ì„ ì‹œë„í–ˆì§€ë§Œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return "í‘í‘... ë‚´ê°€ ì•„ëŠ” ëª¨ë“  ë°©ë²•ì„ ì¨ë´¤ëŠ”ë°ë„ ì§€ê¸ˆì€ ëŒ€ë‹µí•˜ê¸°ê°€ ë„ˆë¬´ ì–´ë ¤ì›Œ, ì„ ìƒ... ì •ë§ ë¯¸ì•ˆí•´, ë‚˜ì¤‘ì— ë‹¤ì‹œ ì°¾ì•„ì™€ ì¤„ë˜?"

        elif is_context_length_issue:
            print(f"ì»¨í…ìŠ¤íŠ¸/ìš”ì²­ í¬ê¸° ë¬¸ì œë¡œ {user_id}ì˜ ì„¸ì…˜ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            if user_id in chat_sessions:
                del chat_sessions[user_id]
            return "ìœ¼ìŒ... ë°©ê¸ˆ ë¬´ìŠ¨ ì´ì•¼ê¸° í•˜ê³  ìˆì—ˆì§€? ë‹¤ì‹œ ë§í•´ì¤„ë˜, ì„ ìƒ? ë¨¸ë¦¬ê°€ ì ê¹ í•˜ì–˜ì¡Œì–´~"
        elif is_safety_issue:
            return "ìœ¼ìŒ... ì„ ìƒ, ê·¸ê±´ ì¢€ ëŒ€ë‹µí•˜ê¸° ê³¤ë€í•œ ë‚´ìš©ì¸ ê²ƒ ê°™ì•„. ë‹¤ë¥¸ ì´ì•¼ê¸° í•˜ì~"
        elif is_api_key_issue:
            print("ì¹˜ëª…ì  ì˜¤ë¥˜: Gemini API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë´‡ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
            return "ìœ¼ì•™... ì„ ìƒë‹˜ì´ë‘ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ë°, ë­”ê°€ ì¤‘ìš”í•œ ì—°ê²°ì´ ëŠì–´ì§„ ê²ƒ ê°™ì•„... ê´€ë¦¬ì ì•„ì €ì”¨í•œí…Œ í•œë²ˆ ë¬¼ì–´ë´ ì¤„ë˜?"
        
        # ì´ ë¶€ë¶„ì€ ì´ë¡ ìƒ ìœ„ì˜ ì¡°ê±´ë“¤(should_switch_model í¬í•¨)ì— ì˜í•´ ì»¤ë²„ë˜ì–´ì•¼ í•˜ì§€ë§Œ,
        # ë§Œì•½ì„ ìœ„í•œ ìµœì¢… fallback (ì‚¬ì‹¤ìƒ ìœ„ì˜ should_switch_modelì´ Trueê°€ ë˜ì–´ ëª¨ë¸ ì „í™˜ì„ ì‹œë„í•  ê²ƒì„)
        print(f"ì²˜ë¦¬ë˜ì§€ ì•Šì€ íŠ¹ì • ì˜ˆì™¸ ìœ í˜•ì…ë‹ˆë‹¤. ì˜¤ë¥˜: {error_type} - {error_message}")
        return "ìš°ì›…... ì§€ê¸ˆì€ ì¢€ í”¼ê³¤í•´ì„œ ëŒ€ë‹µí•˜ê¸° ì–´ë µë„¤~ ë‚˜ì¤‘ì— ë‹¤ì‹œ ë¬¼ì–´ë´ì¤˜, êµ¬ë§Œ."


def initialize_model_globally():
    """ë´‡ ì‹œì‘ ë˜ëŠ” í•„ìš”ì‹œ ì „ì—­ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global current_model_index # current_model_indexë¥¼ ì—…ë°ì´íŠ¸í•˜ë¯€ë¡œ global ì„ ì–¸
    initial_model_loaded = False
    
    # MODEL_NAMES ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ìŠµë‹ˆë‹¤.
    for idx in range(len(MODEL_NAMES)):
        if initialize_model(idx): # initialize_modelì´ current_model_indexë¥¼ ì„¤ì •
            initial_model_loaded = True
            break
        else:
            print(f"{MODEL_NAMES[idx]} ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª¨ë¸ì„ ì‹œë„í•©ë‹ˆë‹¤.")
    
    if not initial_model_loaded:
        print("ì¹˜ëª…ì  ì˜¤ë¥˜: ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ì„ ì°¾ê±°ë‚˜ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. MODEL_NAMES ë¦¬ìŠ¤íŠ¸ì™€ API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    return initial_model_loaded


@bot.event
async def on_ready():
    print(f'ë´‡ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œê·¸ì¸í–ˆìŠµë‹ˆë‹¤: {bot.user.name} (ID: {bot.user.id})')
    
    if not initialize_model_globally():
        print("ë´‡ ì´ˆê¸°í™” ì¤‘ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ë˜ëŠ” ë´‡ì´ ì¢…ë£Œë©ë‹ˆë‹¤.")
        activity = discord.Game(name="ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨! ì ê²€ ì¤‘...")
        await bot.change_presence(status=discord.Status.dnd, activity=activity)
        return
    else:
        print(f"ë´‡ ì¤€ë¹„ ì™„ë£Œ. í˜„ì¬ ì‚¬ìš© ëª¨ë¸: {MODEL_NAMES[current_model_index]}")
        activity = discord.Game(name="!ë„ì›€ ìœ¼ë¡œ ì‚¬ìš©ë²• í™•ì¸!")
        await bot.change_presence(status=discord.Status.online, activity=activity)


@bot.command(name='ì´ˆê¸°í™”')
async def reset_chat_session(ctx: commands.Context):
    """ì‚¬ìš©ìì˜ í˜„ì¬ ëŒ€í™” ì„¸ì…˜ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    user_id = str(ctx.author.id)
    if user_id in chat_sessions:
        del chat_sessions[user_id]
        await ctx.reply(f"ê¸°ì¡´ ëŒ€í™” ë‚´ìš©ì„ ìŠì–´ë²„ë ¸ì–´, ì„ ìƒ! ({MODEL_NAMES[current_model_index]} ëª¨ë¸ê³¼ì˜ ëŒ€í™”ì˜€ì–´.) ìƒˆë¡œìš´ ë§ˆìŒìœ¼ë¡œ ë‹¤ì‹œ ì‹œì‘í•˜ì~ í›„í›„.", mention_author=False)
        print(f"ì±„íŒ… ì„¸ì…˜ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤: {user_id} (ìš”ì²­ì: {ctx.author.name})")
    else:
        await ctx.reply("ì‘? ì•„ì§ ìš°ë¦¬ ëŒ€í™” ì‹œì‘ë„ ì•ˆ í•œ ê²ƒ ê°™ì€ë°, ì„ ìƒ? ì•„ë‹ˆë©´ ì´ë¯¸ ê¹¨ë—í•œ ìƒíƒœì•¼!", mention_author=False)
        print(f"ì´ˆê¸°í™” ìš”ì²­: ì´ë¯¸ ì„¸ì…˜ì´ ì—†ê±°ë‚˜ ì´ˆê¸°í™”ëœ ìƒíƒœì…ë‹ˆë‹¤: {user_id} (ìš”ì²­ì: {ctx.author.name})")

@bot.command(name='ë„ì›€')
async def show_help(ctx: commands.Context):
    """ë´‡ ì‚¬ìš©ë²•ì— ëŒ€í•œ ë„ì›€ë§ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."""
    embed = discord.Embed(
        title="ğŸ“˜ í˜¸ì‹œë…¸ ë´‡ ë„ì›€ë§ ğŸ“˜",
        description="ìœ¼í—¤~í˜¸ì‹œë…¸ëŠ” ì´ëŸ°ê±¸ í• ì¤„ ì•ˆë‹¤êµ¬ ì„ ìƒ!",
        color=discord.Color.blue()
    )
    embed.set_thumbnail(url=bot.user.display_avatar.url)

    embed.add_field(
        name="ğŸ’¬ ì €ì™€ ëŒ€í™”í•˜ê¸°",
        value=f"ì±„ë„ì—ì„œ ì €ë¥¼ **ë©˜ì…˜**(`@{bot.user.name}`)í•˜ê±°ë‚˜ **DM**ìœ¼ë¡œ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ë©´ ëŒ€ë‹µí•´ë“œë ¤ìš”!\n"
            f"ì˜ˆ: `@{bot.user.name} ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?`",
        inline=False
    )
    embed.add_field(
        name="â˜€ï¸ ë‚ ì”¨ ë¬¼ì–´ë³´ê¸°",
        value="`ë„ì‹œì´ë¦„ ë‚ ì”¨`ë¼ê³  ë¬¼ì–´ë³´ì„¸ìš”. (ì˜ˆ: `ì„œìš¸ ë‚ ì”¨`, `ë¶€ì‚° ë‚ ì”¨`)\n"
            "ê·¸ëƒ¥ `ë‚ ì”¨`ë¼ê³  ë¬¼ì–´ë³´ë©´ ì œê°€ ì„ì˜ë¡œ ì„œìš¸ ë‚ ì”¨ë¥¼ ì•Œë ¤ë“œë ¤ìš”.\n"
            "(ë‹¨, ì œê°€ ì•„ëŠ” ë„ì‹œì—¬ì•¼ í•´ìš”!)",
        inline=False
    )
    embed.add_field(
        name="ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”",
        value="`!ì´ˆê¸°í™”` ë¼ê³  ì…ë ¥í•˜ë©´ ì €ì™€ì˜ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ìŠì–´ë²„ë¦¬ê³  ìƒˆë¡œ ì‹œì‘í•  ìˆ˜ ìˆì–´ìš”.",
        inline=False
    )
    embed.add_field(
        name="ğŸ™‹ ë„ì›€ë§ ë³´ê¸°",
        value="`!ë„ì›€` ì´ë¼ê³  ì…ë ¥í•˜ë©´ ì´ ë„ì›€ë§ì„ ë‹¤ì‹œ ë³¼ ìˆ˜ ìˆì–´ìš”.",
        inline=False
    )
    if gemini_model:
        embed.set_footer(text=f"í˜„ì¬ ì‚¬ìš© ëª¨ë¸: {MODEL_NAMES[current_model_index]}. ê¶ê¸ˆí•œ ê²Œ ìˆë‹¤ë©´ ì–¸ì œë“  ì•„ì €ì”¨ì—ê²Œ ë¬¼ì–´ë³´ë¼êµ¬~.")
    else:
        embed.set_footer(text="ëª¨ë¸ ë¡œë”©ì— ë¬¸ì œê°€ ìˆì–´ìš”... ê¶ê¸ˆí•œ ê²Œ ìˆë‹¤ë©´ ì–¸ì œë“  ì•„ì €ì”¨ì—ê²Œ ë¬¼ì–´ë³´ë¼êµ¬~.")


    await ctx.reply(embed=embed, mention_author=False)
    print(f"ë„ì›€ë§ ìš”ì²­: {ctx.author.name} (ID: {ctx.author.id})")


@bot.event
async def on_message(message: discord.Message):
    if message.author == bot.user:
        return

    await bot.process_commands(message)
    
    ctx = await bot.get_context(message)
    if ctx.command:
        return

    is_mentioned = bot.user.mentioned_in(message)
    is_dm = isinstance(message.channel, discord.DMChannel)

    if is_mentioned or is_dm:
        if not gemini_model:
            print("on_message: gemini_modelì´ Noneì…ë‹ˆë‹¤. ëª¨ë¸ ì´ˆê¸°í™”ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
            if not initialize_model_globally():
                await message.reply("ìœ¼ì•™... ì§€ê¸ˆì€ ì•„ë¬´ ìƒê°ë„ í•  ìˆ˜ê°€ ì—†ì–´... ê´€ë¦¬ì ì•„ì €ì”¨í•œí…Œ SOS ì¢€ ì³ì¤„ë˜?", mention_author=False)
                return
            else:
                print(f"on_message: ëª¨ë¸ ì¬ì´ˆê¸°í™” ì„±ê³µ. í˜„ì¬ ëª¨ë¸: {MODEL_NAMES[current_model_index]}")

        user_id = str(message.author.id)
        
        if is_mentioned:
            processed_content = re.sub(rf'<@!?{bot.user.id}>', '', message.content).strip()
        else: 
            processed_content = message.content.strip()

        if not processed_content:
            if is_mentioned:
                await message.channel.send("ì‘? ë¶ˆë €ì–´, ì„ ìƒ? í›„ì•„ì•”... ë¬´ìŠ¨ ì¼ì´ì•¼?")
            return

        async with message.channel.typing():
            bot_reply = await generate_response(user_id, processed_content)
        
        await message.reply(bot_reply, mention_author=False)


if __name__ == "__main__":
    # MODEL_NAMES ë¦¬ìŠ¤íŠ¸ì—ì„œ Gemma ëª¨ë¸ ì£¼ì„ ì²˜ë¦¬ (genai ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§ì ‘ ì§€ì› ì—¬ë¶€ ë¶ˆí™•ì‹¤)
    # ë§Œì•½ Vertex AI SDK ë“±ì„ ì‚¬ìš©í•œë‹¤ë©´ í•´ë‹¹ ë¶€ë¶„ì€ ë‹¤ë¥´ê²Œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” google.generativeai ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ì¤€ì…ë‹ˆë‹¤.
    if 'gemma-3-27b-it' in MODEL_NAMES:
        print("ì£¼ì˜: 'gemma-3-27b-it' ëª¨ë¸ì€ google.generativeai ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì§ì ‘ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Vertex AIë¥¼ ì‚¬ìš©í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # MODEL_NAMES = [name for name in MODEL_NAMES if name != 'gemma-3-27b-it'] # ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œê±°í•˜ëŠ” ë°©ë²•

    if not DISCORD_TOKEN:
        print(".env íŒŒì¼ì— DISCORD_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    elif not GEMINI_API_KEY:
        print(".env íŒŒì¼ì— GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        try:
            bot.run(DISCORD_TOKEN)
        except discord.errors.LoginFailure:
            print("Discord í† í°ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì˜ DISCORD_TOKENì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        except Exception as e: 
            print(f"ë´‡ ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()